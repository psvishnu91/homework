{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "import logz\n",
    "import scipy.signal\n",
    "import os\n",
    "import torch as tc\n",
    "import time\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from itertools import izip, imap\n",
    "\n",
    "import model_utils as K\n",
    "import crayon\n",
    "\n",
    "from collections import namedtuple\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_nt(dictionary):\n",
    "    return namedtuple('GenericDict', dictionary.keys())(**dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GenericDict(n_iter=500, exp_name='test1', learning_rate=0.005, min_timesteps_per_batch=1000, env_name='CartPole-v0', seed=0, reward_to_go=True, logdir=None, normalize_advantages=True, n_hidden_layers=1, nn_baseline=False, animate=True, max_path_length=None, gamma=1.0, hidden_dim=32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLE_CONFIG = dict_to_nt(\n",
    "    dict(\n",
    "        exp_name='test1',\n",
    "        env_name='CartPole-v0',\n",
    "        n_iter=500,\n",
    "        gamma=1.0,\n",
    "        min_timesteps_per_batch=1000,\n",
    "        max_path_length=None,\n",
    "        learning_rate=5e-3,\n",
    "        reward_to_go=True,\n",
    "        animate=True,\n",
    "        logdir=None,\n",
    "        normalize_advantages=True,\n",
    "        nn_baseline=False,\n",
    "        n_hidden_layers=1,\n",
    "        hidden_dim=32,\n",
    "        seed=0,\n",
    "    ),\n",
    ")\n",
    "SAMPLE_CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ModelOpt = namedtuple('ModelOpt', ['model', 'optimizer'])\n",
    "\n",
    "\n",
    "def pathlength(path):\n",
    "    return len(path.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(\n",
    "    D_in, \n",
    "    D_out,\n",
    "    n_hidden_layers=2, \n",
    "    H=64, \n",
    "    activation=tc.nn.LeakyReLU,\n",
    "    output_activation=None,\n",
    "):\n",
    "    layers = (\n",
    "        [tc.nn.Linear(D_in, H), activation()] +\n",
    "        ([tc.nn.Linear(H, H), activation()] * (n_hidden_layers - 1)) +\n",
    "        [tc.nn.Linear(H, D_out)]\n",
    "    )\n",
    "    if output_activation is not None:\n",
    "        layers.append(output_activation())\n",
    "    return tc.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POLICY GRADIENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EnvDetails = namedtuple(\n",
    "    'EnvDetails',\n",
    "    ['env', 'discrete', 'max_path_length', 'ob_dim', 'axn_dim'],\n",
    ")\n",
    "\n",
    "def setup_env(env_name, max_path_length):\n",
    "    # Make the gym environment\n",
    "    env = gym.make(env_name)\n",
    "    discrete = isinstance(env.action_space, gym.spaces.Discrete)\n",
    "    return EnvDetails(\n",
    "        env=env,\n",
    "        # Is this env continuous, or discrete?\n",
    "        discrete=discrete,\n",
    "        # Maximum length for episodes\n",
    "        max_path_length=max_path_length or env.spec.max_episode_steps,\n",
    "        # Observation and action sizes\n",
    "        ob_dim=env.observation_space.shape[0],\n",
    "        axn_dim=env.action_space.n if discrete else env.action_space.shape[0],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discrete MLP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DiscModelOut = namedtuple(\n",
    "    'DiscModelOut',\n",
    "    ['axn', 'axn_probas', 'log_axn_probas', 'chosen_axn_log_proba'],\n",
    ")\n",
    "\n",
    "class DiscreteModel(tc.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, D_out, hidden_dim, n_hidden_layers, activation=tc.nn.LeakyReLU, lr=1e-3):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.mlp_model = build_mlp(\n",
    "            output_activation=tc.nn.Softmax,\n",
    "            D_in=D_in,\n",
    "            D_out=D_out,\n",
    "            H=hidden_dim,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            activation=activation,\n",
    "        )\n",
    "        self.is_discrete = True\n",
    "        self.optimizer = None\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, obs, **kwargs):\n",
    "        # obs -> (batch_sz, obs_dim)\n",
    "        # axn_probas -> (batch_sz, axns_dim)\n",
    "        axn_probas = self.mlp_model(K.array_to_variable(obs))\n",
    "        # axn -> (batch_sz, 1)\n",
    "        axn = axn_probas.multinomial()\n",
    "        # axn_log_proba\n",
    "        log_axn_probas = tc.log(axn_probas)\n",
    "        return DiscModelOut(\n",
    "            axn=axn,\n",
    "            axn_probas=axn_probas,\n",
    "            log_axn_probas=tc.log(axn_probas),\n",
    "            chosen_axn_log_proba=log_axn_probas.gather(1, axn.view(-1,1)),\n",
    "        )\n",
    "    \n",
    "    def update_params(self, paths, reward_to_go, nn_baseline, gamma=1.):\n",
    "        \"\"\"Assumes the rewards following the terminal state is 0.\"\"\"\n",
    "        if not reward_to_go:\n",
    "            raise NotImplementedError\n",
    "        optimizer = self._get_optimizer()\n",
    "        rewards = compute_loss_rtg(paths=paths, gamma=gamma, nn_baseline=True)\n",
    "        max_pathlen = get_max_pathlen(paths)\n",
    "        loss = 0\n",
    "        for i, p in enumerate(paths):\n",
    "            model_outs = self.forward(p.obs)\n",
    "            chosen_log_probas = model_outs.log_axn_probas.gather(1, K.array_to_variable(p.axns).long().view(-1, 1))\n",
    "            loss -= (chosen_log_probas * K.array_to_variable(rewards[i,:len(p)])).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # utils.clip_grad_norm(self.parameters(), 40)\n",
    "        optimizer.step()\n",
    "        return loss.data[0]\n",
    "        \n",
    "    def _get_optimizer(self):\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "        else:\n",
    "            self.optimizer = tc.optim.Adam(params=self.parameters(), lr=self.lr)\n",
    "            return self.optimizer\n",
    "\n",
    "\n",
    "def model_out_to_axn(model_out):\n",
    "    return model_out.axn.data[0][0]\n",
    "\n",
    "\n",
    "def compute_loss_rtg(paths, gamma, nn_baseline):\n",
    "    num_paths = len(paths)\n",
    "    max_pathlen = get_max_pathlen(paths)\n",
    "    # dim (num_paths, max_episode_len)\n",
    "    path_returns = np.zeros((num_paths, max_pathlen))\n",
    "    #     print (\n",
    "    #         'num_paths: {p}, max_pathlen: {m}'.format(\n",
    "    #             p=num_paths,\n",
    "    #             m=max_pathlen,\n",
    "    #         )\n",
    "    #     )\n",
    "    for i, p in enumerate(paths):\n",
    "        path_returns[i] = _compute_returns_rtg(\n",
    "            path=p,\n",
    "            gamma=gamma,\n",
    "            max_pathlen=max_pathlen,\n",
    "        )\n",
    "    if nn_baseline:\n",
    "        path_returns = (\n",
    "            (path_returns - path_returns.mean(axis=0)) /\n",
    "            (path_returns.std(axis=0) + np.finfo(np.float32).eps)\n",
    "        )\n",
    "    return path_returns\n",
    "\n",
    "\n",
    "def get_max_pathlen(paths):\n",
    "    return max(imap(len, paths))\n",
    "\n",
    "def _compute_returns_rtg(path, gamma, max_pathlen):\n",
    "    \"\"\"Returns reward to go returns for each time step\"\"\"\n",
    "    Rs = np.zeros((1, max_pathlen))\n",
    "    R = 0\n",
    "    for i in reversed(xrange(len(path))):\n",
    "        R = path.rewards[i] + gamma * R\n",
    "        Rs[0, i] = R\n",
    "    return Rs\n",
    "\n",
    "\n",
    "def extract_model_out_field(path, field):\n",
    "    return map(lambda mo: getattr(mo, field), path.model_outs)\n",
    "\n",
    "\n",
    "def extract_chosen_axn_log_proba(path, pad_to_size=None):\n",
    "    var = tc.cat(extract_model_out_field(path=path, field='chosen_axn_log_proba'), dim=1)\n",
    "    if not pad_to_size or pad_to_size==var.size()[1]:\n",
    "        return var\n",
    "    return tc.cat([var, tc.zeros(1, pad_to_size - len(path.model_outs))],dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinousModel(tc.nn.Module):\n",
    "    \n",
    "    def __init__(self, D_in, D_out, hidden_dim, n_hidden_layers, activation=tc.nn.LeakyReLU, lr=1e-3):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.mu = build_mlp(\n",
    "            output_activation=None,\n",
    "            D_in=D_in,\n",
    "            D_out=D_out,\n",
    "            H=hidden_dim,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            activation=activation,\n",
    "        )\n",
    "        # Assume a diagonal covariance\n",
    "        self.z = K.CudableVariable(tc.randn(1, D_out)).float()\n",
    "        self.is_discrete = False\n",
    "        self.optimizer = None\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, obs, **kwargs):\n",
    "        # obs -> (batch_sz, obs_dim)\n",
    "        # axn_probas -> (batch_sz, axns_dim)\n",
    "        mu = self.mu(K.array_to_variable(obs))\n",
    "        # axn -> (batch_sz, 1)\n",
    "        axn = mu + (self.z * tc.normal(K.placeholder(mu.size())))\n",
    "        # axn_log_proba\n",
    "        return dict_to_nt(dict(axn=axn, mu=mu))\n",
    "    \n",
    "    def update_params(self, paths, reward_to_go, nn_baseline, gamma=1.):\n",
    "        \"\"\"Assumes the rewards following the terminal state is 0.\"\"\"\n",
    "        if not reward_to_go:\n",
    "            raise NotImplementedError\n",
    "        optimizer = self._get_optimizer()\n",
    "        rewards = compute_loss_rtg(paths=paths, gamma=gamma, nn_baseline=True)\n",
    "        max_pathlen = get_max_pathlen(paths)\n",
    "        loss = 0\n",
    "        for i, p in enumerate(paths):\n",
    "            model_outs = self.forward(p.obs)\n",
    "            loss += (\n",
    "                0.5 *\n",
    "                (1 / tc.pow(self.z, 2)) *\n",
    "                tc.pow( (K.array_to_variable(p.axns) - model_outs.axn), 2) *\n",
    "                K.array_to_variable(rewards[i,:len(p)])\n",
    "            ).sum()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # utils.clip_grad_norm(self.parameters(), 40)\n",
    "        optimizer.step()\n",
    "        return loss.data[0]\n",
    "        \n",
    "    def _get_optimizer(self):\n",
    "        if self.optimizer is not None:\n",
    "            return self.optimizer\n",
    "        else:\n",
    "            self.optimizer = tc.optim.Adam(params=self.parameters(), lr=self.lr)\n",
    "            return self.optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(env_details, n_hidden_layers, hidden_dim, lr=1e-3):\n",
    "    if not env_details.discrete:\n",
    "        return ContinousModel(\n",
    "            D_in=env_details.ob_dim,\n",
    "            D_out=env_details.axn_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            lr=lr,\n",
    "        )\n",
    "    else:\n",
    "        return DiscreteModel(\n",
    "            D_in=env_details.ob_dim,\n",
    "            D_out=env_details.axn_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            n_hidden_layers=n_hidden_layers,\n",
    "            lr=lr,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_state(env, step=0, info='', render_img=None):\n",
    "    \"\"\"Show cart pole inline.\n",
    "    \n",
    "    Source: https://stackoverflow.com/a/45179251/1950328\n",
    "    \"\"\"\n",
    "    plt.figure(3)\n",
    "    if render_img is None:\n",
    "        render_img = plt.imshow(env.render(mode='rgb_array'))\n",
    "    else:\n",
    "        render_img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "    plt.title(\"%s | Step: %d %s\" % (env.spec.id, step, info))\n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "    return render_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Path(namedtuple('Path', ['obs', 'rewards', 'axns', 'model_outs'])):\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rewards)\n",
    "\n",
    "\n",
    "def rollout(model, env_details, animate, min_timesteps_per_batch):\n",
    "    total_timesteps = 0\n",
    "    env = env_details.env\n",
    "    timesteps_this_batch = 0\n",
    "    paths = []\n",
    "    render_img = None\n",
    "    while True:\n",
    "        ob = env.reset()\n",
    "        obs, axns, rewards, model_outs = [], [], [], []\n",
    "        animate_this_episode=(animate)\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if animate_this_episode:\n",
    "                render_img = show_state(\n",
    "                    env=env,\n",
    "                    step=steps,\n",
    "                    render_img=render_img,\n",
    "                )\n",
    "                time.sleep(0.05)\n",
    "            obs.append(ob)\n",
    "            model_out = model(ob[None])\n",
    "            model_outs.append(model_out)\n",
    "            axn = model_out_to_axn(model_out=model_out)\n",
    "            axns.append(axn)\n",
    "            ob, rew, done, _ = env.step(axn)\n",
    "            rewards.append(rew)\n",
    "            steps += 1\n",
    "            if done or steps > env_details.max_path_length:\n",
    "                break\n",
    "        path = Path(\n",
    "            obs=np.array(obs), \n",
    "            rewards=np.array(rewards), \n",
    "            axns=np.array(axns),\n",
    "            model_outs=model_outs,\n",
    "        )\n",
    "        paths.append(path)\n",
    "        timesteps_this_batch += pathlength(path)\n",
    "        if timesteps_this_batch > min_timesteps_per_batch:\n",
    "            break\n",
    "    total_timesteps += timesteps_this_batch\n",
    "    env_details.env.close()\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward_per_episode(paths):\n",
    "    return sum(imap(lambda p: p.rewards.sum(), paths)) / len(paths)\n",
    "\n",
    "\n",
    "def avg_path_len(paths):\n",
    "    return sum(imap(len, paths)) / len(paths)\n",
    "\n",
    "\n",
    "def train_env(env_name, expt_name, config=SAMPLE_CONFIG):\n",
    "    env_details = setup_env(env_name=env_name, max_path_length=config.max_path_length)\n",
    "    print 'Env_details:', env_details\n",
    "    model = create_model(\n",
    "        env_details=env_details,\n",
    "        n_hidden_layers=config.n_hidden_layers,\n",
    "        hidden_dim=config.hidden_dim,\n",
    "    )\n",
    "    print 'Constructed Model: ', model\n",
    "    expt = crayon.get_experiment(name=env_name)\n",
    "    pbar = K.Progbar(target=config.n_iter)\n",
    "    for i in xrange(config.n_iter):\n",
    "        paths = rollout(\n",
    "            model=model,\n",
    "            env_details=env_details,\n",
    "            animate=False,\n",
    "            min_timesteps_per_batch=config.min_timesteps_per_batch,\n",
    "        )\n",
    "        loss = model.update_params(\n",
    "            paths=paths,\n",
    "            reward_to_go=True,\n",
    "            nn_baseline=True,\n",
    "            gamma=config.gamma,\n",
    "        )\n",
    "        # Losses\n",
    "        expt.add_scalar_dict(\n",
    "            {\n",
    "                'loss': loss,\n",
    "                'reward_per_episode': compute_reward_per_episode(paths),\n",
    "                'avg_path_len': avg_path_len(paths),\n",
    "            },\n",
    "        )\n",
    "        pbar.update(i)\n",
    "    return dict_to_nt(dict(env_details=env_details, model=model))\n",
    "\n",
    "\n",
    "def sim_trained_model(env_details, model, min_timesteps_per_batch=200):\n",
    "    rollout(\n",
    "        model=model,\n",
    "        env_details=env_details,\n",
    "        animate=True,\n",
    "        min_timesteps_per_batch=min_timesteps_per_batch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crayon.clear_expts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cart-Pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env_details: EnvDetails(env=<TimeLimit<CartPoleEnv<CartPole-v0>>>, discrete=True, max_path_length=200, ob_dim=4, axn_dim=2)\n",
      "Constructed Model:  DiscreteModel (\n",
      "  (mlp_model): Sequential (\n",
      "    (0): Linear (4 -> 32)\n",
      "    (1): LeakyReLU (0.01)\n",
      "    (2): Linear (32 -> 2)\n",
      "    (3): Softmax ()\n",
      "  )\n",
      ")\n",
      "499/500 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "model_cp = train_env(\n",
    "    env_name='CartPole-v0',\n",
    "    expt_name='cart_pole_pg',\n",
    "    config=dict_to_nt(\n",
    "        dict(\n",
    "            n_iter=500,\n",
    "            gamma=0.99,\n",
    "            min_timesteps_per_batch=1000,\n",
    "            max_path_length=None,\n",
    "            learning_rate=5e-3,\n",
    "            reward_to_go=True,\n",
    "            nn_baseline=True,\n",
    "            n_hidden_layers=1,\n",
    "            hidden_dim=32,\n",
    "        ),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
